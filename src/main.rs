#![feature(backtrace)]

extern crate image;

use std::fs::File;
use std::io::{Read, Write};
use std::iter::Iterator;
use std::path::Path;

use curl::easy::Easy;
use docopt::Docopt;
use flate2::read::GzDecoder;
use serde::Deserialize;
use url::Url;

use crate::error::Error;

use juice::layer::{LayerConfig, LayerType, Layer};
use juice::layers::{LinearConfig, NegativeLogLikelihoodConfig, SequentialConfig};

use juice::util::{native_backend, write_batch_sample};

use juice::solver::{Solver, SolverConfig, ConfusionMatrix};
use std::rc::Rc;

use byteorder::{BigEndian, ByteOrder, ReadBytesExt};
use std::iter::FromIterator;
use crate::dataIter::{DataIter, BatchDataIter};
use coaster::{SharedTensor, Backend, Native};
use std::sync::{Arc, RwLock};
use std::borrow::BorrowMut;

mod error;
mod dataIter;

type BatchVec = Vec<(u8, Vec<u8>)>;

const DATA_LINKS: [&str; 4] = [
    "http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz",
    "http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz",
    "http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz",
    "http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz",
];

const USAGE: &'static str = "
Juice MNIST Example

Usage:
juice_mnist download <directory>
juice_mnist train <batch> <file> [<hidden>...] [--learning-rate=<rate>] [--train-loops=<loops>]
juice_mnist test <config> <batch>
juice_mnist (-h | --help)

Commands:
  download                            Downloads the datasets
  train <batch> <file> [<hidden>...]        Trains the network and saves it based on the data in the \"data\" directory
  test <config> <batch>                      Tests the saved data based on the passed in config generated by --train.

Options:
  -h --help                 Show this screen.
  --learning-rate=<rate>    Learning rate that's used by the model.
  --train-loops=<loops>     How many times we should loop over the training set
";

#[derive(Debug, Deserialize)]
struct Args {
    cmd_download: bool,
    cmd_train: bool,
    cmd_test: bool,
    arg_directory: Option<String>,
    arg_config: Option<String>,
    arg_batch: Option<usize>,
    arg_hidden: Option<Vec<usize>>,
    arg_file: Option<String>,
    flag_learning_rate: Option<f32>,
    flag_train_loops: Option<usize>,
}

fn download_datasets(dir: Option<String>) -> Result<(), Error> {
    let directory = dir.ok_or("Please specify a directory when downloading datasets")?;

    let mut easy = Easy::new();

    for s in DATA_LINKS.iter() {
        let s = *s;
        let url = Url::parse(s)?;
        let filename: &str = url
            .path_segments()
            .ok_or("Could not get path segments")?
            .last()
            .ok_or("Could not get last path segment")?;

        let path = Path::new::<str>(directory.as_ref()).join(filename);
        let path = path.as_path();
        println!("Downloading {:?}", path.file_name().unwrap());
        {
            let mut file = File::create(path).expect("Failed to create a file to write to");

            easy.url(s)?;
            easy.write_function(move |data| {
                file.write_all(&data).unwrap();
                Ok(data.len())
            })?;
            easy.perform().unwrap();
        }

        {
            let mut gzippedBytes: Vec<u8> = vec![];
            let file = File::open(path)?;
            let mut decoder = GzDecoder::new(file);
            println!("Decoding file {:?}", path.file_name().unwrap());
            decoder.read_to_end(&mut gzippedBytes);

            // Write the decoded file
            let path = Path::new::<str>(directory.as_ref()).join(filename.replace(".gz", ""));
            let mut unzipped_file = File::create(path)?;
            unzipped_file.write_all(gzippedBytes.as_slice())?;
        }
    }
    Ok(())
}

fn readu32(file: &File) -> Result<u32, Error> {
    let mut values = [0u8; 4];
    let mut taker = file.take(4);
    taker.read(&mut values)?;

    Ok(BigEndian::read_u32(&values))
}

fn get_image_iterator(
    label_file_path: &Path,
    image_file_path: &Path,
    verification_numbers: (u32, u32),
    batch_size: usize,
	loops: usize,
) -> Result<((usize, usize), Vec<BatchVec>), Error> {
    let mut rows = 0;
    let mut cols = 0;

   let zipped_iterator: Vec<BatchVec>= (0..loops).flat_map(|_| -> BatchDataIter {
        let mut label_file = File::open(label_file_path).unwrap();
        let mut image_file = File::open(image_file_path).unwrap();

        // These magic numbers verify that you are reading bits the right way
        assert_eq!(readu32(&label_file).unwrap(), verification_numbers.0);
        assert_eq!(readu32(&image_file).unwrap(), verification_numbers.1);

        let num_of_records = readu32(&label_file).unwrap();
        assert_eq!(num_of_records, readu32(&image_file).unwrap());

        rows = readu32(&image_file).unwrap() as usize;
        cols = readu32(&image_file).unwrap() as usize;

        println!("Image dimensions are: ({}, {})", rows, cols);

        let data_iter: DataIter = DataIter {
            label_file,
            image_file,
            rows,
            cols
        };
        let batch_data_iter = BatchDataIter { data_iter, batch_size };
	    batch_data_iter
    }).collect::<Vec<BatchVec>>();
    Ok(((rows, cols), zipped_iterator))
}

fn get_model_cfg(batch_size: usize, input_dimensions: (usize, usize), hidden_layer_sizes: Vec<usize>) -> LayerConfig {
    let linear = LayerConfig::new("last_linear", LinearConfig { output_size: 10 });

    let last_activation = LayerConfig::new("softmax", LayerType::LogSoftmax);

    let mut total_config = SequentialConfig::default();
    total_config.add_input(
        "data",
        &vec![batch_size, input_dimensions.0, input_dimensions.1],
    );
    for (i, &hidden_size) in hidden_layer_sizes.iter().enumerate() {
        let layer = LayerConfig::new(format!("layer_{}", i).as_str(), LinearConfig { output_size: hidden_size });
        total_config.add_layer(layer);
    }
    total_config.add_layer(linear);
    total_config.add_layer(last_activation);

    LayerConfig::new("model", LayerType::Sequential(total_config))
}

fn get_solver_cfg(batch_size: usize, output_size: usize) -> LayerConfig {
    let nll_config = LayerConfig::new(
        "neg. log likelihood",
        NegativeLogLikelihoodConfig {
            num_classes: output_size,
        },
    );

    let mut config = SequentialConfig::default();
    config.add_input("network_output", &[batch_size, output_size]);
    config.add_input("label", &[batch_size, 1]);
    config.add_layer(nll_config);

    LayerConfig::new("solver", LayerType::Sequential(config))
}

fn train_dataset(
    batch_size: Option<usize>,
    hidden_layer_size: Option<Vec<usize>>,
    file: Option<String>,
    learning_rate: Option<f32>,
    times_to_loop: Option<usize>,
) -> Result<(), Error> {
    let train_label_path = Path::new("data/train-labels-idx1-ubyte");
    let train_image_path = Path::new("data/train-images-idx3-ubyte");

    // Put this in a command param
    let batch_size = batch_size.unwrap_or(10);
    let output_size = 10usize;

    let ((rows, cols), mut train_data) = get_image_iterator(
        train_label_path,
        train_image_path,
        (2049, 2051),
        batch_size,
        times_to_loop.unwrap_or(1)
    )?;

    let network_cfg = get_model_cfg(batch_size, (rows, cols), hidden_layer_size.unwrap_or(vec![500]));
    let object_cfg = get_solver_cfg(batch_size, output_size);

    let mut solver_cfg = SolverConfig::default();
    solver_cfg.network = network_cfg;
    solver_cfg.objective = object_cfg;
    solver_cfg.base_lr = learning_rate.unwrap_or(0.1);

    let native_backend = Rc::new(native_backend());
    let mut solver = Solver::from_config(native_backend.clone(), native_backend.clone(), &solver_cfg);

    let mut label_tensor: SharedTensor<f32> = SharedTensor::new(&[batch_size, 1]);
	let mut inp_tensor: SharedTensor<f32> = SharedTensor::new(&[batch_size, rows*cols]);

    let mut label_lock = Arc::new(RwLock::new(label_tensor));
    let mut inp_lock = Arc::new(RwLock::new(inp_tensor));

    let mut confusion = ConfusionMatrix::new(output_size);

    let times_to_loop = times_to_loop.unwrap_or(1);
    println!("times_to_loop = {}", times_to_loop);
    for batch in train_data.iter() {
        for i in 0..batch_size {
            let mut labels = label_lock.write()?;
            let mut inputs = inp_lock.write()?;
            write_batch_sample(&mut labels, &[batch.get(i).unwrap().0], i);
            write_batch_sample(&mut inputs, batch.get(i).unwrap().1.as_slice(), i);
        }

        let inferred = solver.train_minibatch(inp_lock.clone(), label_lock.clone());
        let mut inferred = inferred.write()?;
        let predictions = confusion.get_predictions(&mut inferred);
        let labels = Vec::from_iter(batch.iter().map(|(u, v)| {
            *u as usize
        }));

        // println!("  Predictions were: {:?}", &predictions);
        // println!("Actual Labels were: {:?}", &labels);

        confusion.add_samples(&predictions, &labels);
    }
    println!("Accuracy: {}", confusion.accuracy());

    if let Some(f) = file {
        println!("Writing network to {}", f);
        solver.mut_network().save(f);
    }

    Ok(())
}

fn test_dataset(path: Option<String>, batch_size: Option<usize>) -> Result<(), Error> {
    let test_label_path = Path::new("data/t10k-labels-idx1-ubyte");
    let test_image_path = Path::new("data/t10k-images-idx3-ubyte");

    let backend = Rc::new(native_backend());
    let mut network: Layer<Backend<Native>> =  Layer::<Backend<Native>>::load(backend, path.unwrap())?;

    let batch_size = batch_size.unwrap_or(10);

    let mut confusion_matrix = ConfusionMatrix::new(10);

    let ((rows, cols), mut test_data) = get_image_iterator(
        test_label_path,
        test_image_path,
        (2049, 2051),
        batch_size,
        1,
    )?;

	let mut label_tensor = SharedTensor::<f32>::new(&[batch_size, 1]);
    let mut image_tensor = SharedTensor::<f32>::new(&[batch_size, rows*cols]);

    let mut label_lock = Arc::new(RwLock::new(label_tensor));
    let mut image_lock = Arc::new(RwLock::new(image_tensor));

    for batch in test_data {
	    let mut labels = vec![];
        for (i, (label, data)) in batch.iter().enumerate() {
            labels.push(*label as usize);
            let mut labels = label_lock.write()?;
	        let mut images = image_lock.write()?;

            write_batch_sample(&mut labels, &[*label as usize], i);
            write_batch_sample(&mut images, data.as_slice(), i);
        }
        let results_vec = network.forward(&[image_lock.clone()]);

        let mut results = results_vec.get(0).unwrap().write()?;
        let predictions = confusion_matrix.get_predictions(&mut results);

        confusion_matrix.add_samples(predictions.as_slice(), labels.as_slice());
    }

    println!("Accuracy is {}", confusion_matrix.accuracy());

    Ok(())
}

fn main() -> Result<(), Error> {
    let args: Args = Docopt::new(USAGE)
        .and_then(|d| d.deserialize())
        .unwrap_or_else(|e| e.exit());

    println!("{:?}", args);

    if args.cmd_download {
        download_datasets(args.arg_directory)?;
    } else if args.cmd_train {
        train_dataset(args.arg_batch, args.arg_hidden, args.arg_file, args.flag_learning_rate, args.flag_train_loops)?;
    } else if args.cmd_test {
        test_dataset(args.arg_config, args.arg_batch)?;
    }

    Ok(())
}
